{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "a448c8e6-daa1-4dcf-bebc-2b6cc6bdd25c",
   "metadata": {},
   "source": [
    "Q1. Explain the assumptions required to use ANOVA and provide examples of violations that could impact the validity of the results."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf35af-a3b8-4d6b-b268-8f7ba4cb8085",
   "metadata": {},
   "source": [
    "Ans. Analysis of Variance (ANOVA) is a statistical technique used to compare the means of two or more groups to determine whether there are statistically significant differences between them. To use ANOVA effectively and ensure the validity of the results, several assumptions must be met. These assumptions are related to the data and the underlying statistical model. Here are the key assumptions for ANOVA and examples of violations that can impact the validity of the results:\n",
    "\n",
    "Independence: The observations in each group are assumed to be independent of each other. Violations can occur if there is dependency between observations. For example, if you're measuring the test scores of students in different classes and some students appear in multiple classes, this assumption could be violated.\n",
    "\n",
    "Homogeneity of Variance (Homoscedasticity): The variances of the groups being compared should be approximately equal. Violations can lead to inaccurate p-values and compromised validity. For instance, if you're comparing the income levels of different age groups, and one age group shows much larger income variability than others, this assumption may be violated.\n",
    "\n",
    "Normality of Residuals: The residuals (the differences between the observed values and the group means) should follow a normal distribution. Violations may lead to incorrect conclusions, especially in small sample sizes. If the residuals are not normally distributed, it can affect the p-values and confidence intervals. For example, in a study comparing the heights of individuals from different regions, if the residuals do not follow a normal distribution, ANOVA results may be unreliable.\n",
    "\n",
    "Random Sampling: The data should be collected through a random sampling process to make inferences about the population. If the sampling process is not random, it could introduce bias into the results. For instance, if you're comparing the preferences of customers who voluntarily participate in a survey versus those who don't, non-random sampling can lead to biased results.\n",
    "\n",
    "Independence of Errors: The errors (residuals) within each group should be independent of each other. Violations can lead to unreliable parameter estimates and incorrect inferences. If you're comparing the reaction times of individuals to different stimuli, and the reaction times within each group are dependent on each other, this assumption may be violated.\n",
    "\n",
    "Equal Sample Sizes (for one-way ANOVA): In one-way ANOVA (comparing means of three or more groups), having roughly equal sample sizes in each group is preferable. Highly unequal sample sizes can lead to imbalanced statistical power. For example, if you're comparing the performance of different brands of products, having vastly different sample sizes for each brand could affect the results.\n",
    "\n",
    "Continuous Dependent Variable: ANOVA assumes that the dependent variable is measured on a continuous scale. If you're trying to use ANOVA with a categorical dependent variable or one that is not continuous (e.g., nominal or ordinal data), the results may not be valid."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95371f9-d1e2-4003-b04f-90d9aa3fcb40",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "338b7556-0731-4f6a-bbb9-a68029a2e023",
   "metadata": {},
   "source": [
    "Q2. What are the three types of ANOVA, and in what situations would each be used?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9943bc5c-3ed0-4217-a94d-bc87c6ea60de",
   "metadata": {},
   "source": [
    "Ans. Analysis of Variance (ANOVA) is a statistical technique used to compare the means of two or more groups to determine whether there are statistically significant differences among them. There are three main types of ANOVA, each of which is used in specific situations:\n",
    "\n",
    "One-Way ANOVA:\n",
    "It is used when you have one independent variable (factor) with three or more levels (groups) and you want to determine if there are statistically significant differences in the means of the groups.\n",
    "Example: You might use one-way ANOVA to compare the test scores of students who have been taught by three different teachers to see if there are significant differences in the teaching effectiveness of these teachers.\n",
    "\n",
    "Two-Way ANOVA:\n",
    "It is used when you have two independent variables (factors) and you want to examine the influence of both variables on the dependent variable. It allows you to test for main effects of each factor and their interaction effect.\n",
    "Example: You might use two-way ANOVA to study the effects of both diet (factor 1 with levels: low-fat, medium-fat, high-fat) and exercise (factor 2 with levels: sedentary, moderate, intense) on weight loss. This would help you determine the main effects of diet, exercise, and whether there's an interaction effect between them.\n",
    "\n",
    "Multivariate Analysis of Variance (MANOVA):\n",
    "It is used when you have two or more dependent variables and two or more independent variables. It extends ANOVA by examining the combined effects of independent variables on multiple dependent variables simultaneously.\n",
    "Example: If you're studying the impact of different treatment methods on patient outcomes and you're measuring multiple variables like blood pressure, cholesterol levels, and body weight, MANOVA would allow you to analyze the collective influence of the treatments on these variables. It's also used in experimental designs with repeated measures or within-subjects factors, where the same subjects are measured under different conditions."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "584d4c7c-82e3-4608-b912-5a5a80fdea79",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "e991ec36-6f0e-42d0-bd82-f1d123906768",
   "metadata": {},
   "source": [
    "Q3. What is the partitioning of variance in ANOVA, and why is it important to understand this concept?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ca8a16c-9836-4b10-8288-e8e4657fe49b",
   "metadata": {},
   "source": [
    "Ans. The partitioning of variance in Analysis of Variance (ANOVA) is a fundamental concept that breaks down the total variability in the data into different components to help assess the sources of variation and determine whether there are statistically significant differences among groups or factors. Understanding this concept is crucial because it allows researchers to draw meaningful conclusions about the effects of the independent variables on the dependent variable and to test hypotheses.\n",
    "\n",
    "In ANOVA, the total variance in the data is divided into the following components:\n",
    "\n",
    "Total Variance (Total Sum of Squares, SST): This represents the overall variability in the data without considering any grouping or treatment effects. It is calculated as the sum of squared differences between each data point and the overall mean.\n",
    "\n",
    "Between-Group Variance (Between-Groups Sum of Squares, SSB): This component represents the variability that can be attributed to the differences between the group means. It measures the impact of the independent variable(s) or factors on the dependent variable. SSB is calculated as the sum of squared differences between each group's mean and the overall mean, weighted by the number of observations in each group.\n",
    "\n",
    "Within-Group Variance (Within-Groups Sum of Squares, SSW): This component represents the variability that is not explained by the group or treatment effects. It is the sum of squared differences between individual data points and their respective group means.\n",
    "\n",
    "The main idea behind partitioning variance is that if the between-group variance (SSB) is significantly larger than the within-group variance (SSW), it suggests that the independent variable(s) have a statistically significant effect on the dependent variable. This difference is quantified by the F-statistic, which is calculated as the ratio of between-group variance to within-group variance (F = SSB / SSW).\n",
    "\n",
    "Understanding the partitioning of variance is important for several reasons:\n",
    "\n",
    "Hypothesis Testing: ANOVA helps determine whether there are statistically significant differences among groups or factors. By partitioning the variance, you can evaluate the effect of independent variables and test whether these effects are likely due to chance or if they are significant.\n",
    "\n",
    "Identifying Sources of Variation: ANOVA helps researchers understand the sources of variation in the data. It allows you to quantify and attribute the portion of variance associated with different factors or groups.\n",
    "\n",
    "Decision-Making: It assists in making informed decisions about the acceptance or rejection of null hypotheses, helping researchers draw meaningful conclusions about the factors or treatments being studied.\n",
    "\n",
    "Comparing Multiple Groups: ANOVA is particularly useful when comparing means across multiple groups or treatment conditions. It provides a structured and efficient way to assess differences and avoids the problem of inflating Type I error rates associated with multiple pairwise comparisons (e.g., t-tests)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63536601-d206-4ef7-b466-b15dc62beb46",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1f8a7d3e-21cd-4e33-b326-57b69a164847",
   "metadata": {},
   "source": [
    "Q4. How would you calculate the total sum of squares (SST), explained sum of squares (SSE), and residual sum of squares (SSR) in a one-way ANOVA using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d9a18520-65df-49f5-a8f6-0375c5f629fe",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "SST (Total Sum of Squares): 405.33333333333326\n",
      "SSE (Explained Sum of Squares): 298.1333333333334\n",
      "SSR (Residual Sum of Squares): 107.2\n",
      "F-statistic: 16.686567164179113\n",
      "P-value: 0.0003422116504583588\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "group1 = [23, 25, 28, 32, 30]\n",
    "group2 = [18, 20, 22, 19, 25]\n",
    "group3 = [29, 33, 31, 35, 30]\n",
    "data = [group1, group2, group3]\n",
    "\n",
    "all_data = np.concatenate(data)\n",
    "grand_mean = np.mean(all_data)\n",
    "\n",
    "SST = 0\n",
    "SSE = 0\n",
    "SSR = 0\n",
    "\n",
    "for group in data:\n",
    "    group_mean = np.mean(group)\n",
    "    SST += np.sum((group - grand_mean) ** 2)\n",
    "    SSE += len(group) * (group_mean - grand_mean) ** 2\n",
    "    SSR += np.sum((group - group_mean) ** 2)\n",
    "\n",
    "f_statistic, p_value = stats.f_oneway(*data)\n",
    "\n",
    "print(f\"SST (Total Sum of Squares): {SST}\")\n",
    "print(f\"SSE (Explained Sum of Squares): {SSE}\")\n",
    "print(f\"SSR (Residual Sum of Squares): {SSR}\")\n",
    "print(f\"F-statistic: {f_statistic}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "971bed8f-2cdd-4974-980a-6b78d245477d",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "f1ba221c-d9b6-4fd2-b429-426762435a13",
   "metadata": {},
   "source": [
    "Q5. In a two-way ANOVA, how would you calculate the main effects and interaction effects using Python?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "79da2fd1-764a-4475-ba1c-712a617c8cad",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Main Effect of Factor1: F = 0.023255813953486693, p-value = 0.8861768476249439\n",
      "Main Effect of Factor2: F = 12.302325581395353, p-value = 0.024728481466218727\n",
      "Interaction Effect: F = 0.023255813953488337, p-value = 0.8861768476249395\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import statsmodels.api as sm\n",
    "from statsmodels.formula.api import ols\n",
    "\n",
    "data = {\n",
    "    'Factor1': ['A', 'A', 'B', 'B', 'A', 'A', 'B', 'B'],\n",
    "    'Factor2': ['X', 'Y', 'X', 'Y', 'X', 'Y', 'X', 'Y'],\n",
    "    'DependentVariable': [10, 15, 12, 18, 14, 20, 11, 17]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "model = ols('DependentVariable ~ Factor1 * Factor2', data=df).fit()\n",
    "anova_table = sm.stats.anova_lm(model, typ=2)\n",
    "\n",
    "main_effect_Factor1 = anova_table.loc['Factor1', 'F']\n",
    "main_effect_Factor2 = anova_table.loc['Factor2', 'F']\n",
    "interaction_effect = anova_table.loc['Factor1:Factor2', 'F']\n",
    "\n",
    "main_effect_p_value_Factor1 = anova_table.loc['Factor1', 'PR(>F)']\n",
    "main_effect_p_value_Factor2 = anova_table.loc['Factor2', 'PR(>F)']\n",
    "interaction_p_value = anova_table.loc['Factor1:Factor2', 'PR(>F)']\n",
    "\n",
    "print(f\"Main Effect of Factor1: F = {main_effect_Factor1}, p-value = {main_effect_p_value_Factor1}\")\n",
    "print(f\"Main Effect of Factor2: F = {main_effect_Factor2}, p-value = {main_effect_p_value_Factor2}\")\n",
    "print(f\"Interaction Effect: F = {interaction_effect}, p-value = {interaction_p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4eb6dda8-6bb2-41d9-86fd-18b3c7fff122",
   "metadata": {},
   "source": [
    "In the code above, we first fit a two-way ANOVA model using statsmodels and create an ANOVA table. We then extract the F-statistics and p-values for the main effects of Factor1 and Factor2, as well as the interaction effect between the two factors. These values help you determine whether the main effects and interaction effects are statistically significant."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e9eb732c-7b93-4172-b9fb-8cb2385b47d5",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "0926ebfd-71e2-4903-9a55-1e8feaabb564",
   "metadata": {},
   "source": [
    "Q6. Suppose you conducted a one-way ANOVA and obtained an F-statistic of 5.23 and a p-value of 0.02. What can you conclude about the differences between the groups, and how would you interpret these results?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f0b0cbe-30f7-4509-80a2-392f64aac2c2",
   "metadata": {},
   "source": [
    "Ans. In a one-way ANOVA, the F-statistic and associated p-value are used to determine whether there are statistically significant differences between the groups. Here's how to interpret the results you've obtained:\n",
    "\n",
    "F-Statistic: The F-statistic is a measure of the ratio of the variance between the groups to the variance within the groups. In your case, you obtained an F-statistic of 5.23.\n",
    "\n",
    "P-Value: The p-value is a measure of the probability of observing the obtained F-statistic (or a more extreme value) if there were no true differences between the groups. In your case, the p-value is 0.02.\n",
    "\n",
    "Now, let's interpret these results:\n",
    "\n",
    "If the p-value is less than your chosen significance level (e.g., 0.05), you can conclude that there are statistically significant differences between the groups.\n",
    "In your case, the p-value of 0.02 is less than the common significance level of 0.05. This means you have evidence to reject the null hypothesis (H0) and conclude that there are statistically significant differences between the groups. In other words, at least one group is different from the others in terms of the variable you examined.\n",
    "\n",
    "To provide a more detailed interpretation, you can say something like, \"The one-way ANOVA test indicated that there are statistically significant differences between the groups (F(1, N) = 5.23, p = 0.02), where 'F' represents the F-statistic, 'N' is the total number of observations, and 'p' is the p-value. This suggests that at least one of the groups differs from the others in terms of [insert the variable you studied]. Further post hoc tests or pairwise comparisons may be needed to identify which specific groups differ from each other.\"\n",
    "\n",
    "Keep in mind that while the one-way ANOVA tells you that there are differences between the groups, it does not tell you which groups are different from each other. To determine the specific group differences, you may need to perform post hoc tests or pairwise comparisons."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a226f24-13dc-4900-ad0e-7851f0fe87f5",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "2a65c847-c0a3-4a53-b7ac-63dfdcac413c",
   "metadata": {},
   "source": [
    "Q7. In a repeated measures ANOVA, how would you handle missing data, and what are the potential consequences of using different methods to handle missing data?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b330ce9c-48a8-4e2b-92c4-c37b87bc06d2",
   "metadata": {},
   "source": [
    "Ans. Handling missing data in a repeated measures ANOVA is an important consideration to ensure the validity and reliability of your analysis. Missing data can arise for various reasons, such as participant dropout or measurement errors. There are several methods to handle missing data in a repeated measures ANOVA, and the choice of method can impact your results and conclusions. Here's an overview of handling missing data and the potential consequences of different approaches:\n",
    "\n",
    "Listwise Deletion (Complete Case Analysis):\n",
    "In listwise deletion, cases with missing data on any variable are completely removed from the analysis. This is a straightforward approach, but it can result in a loss of statistical power and may introduce bias if the missing data is not completely at random.\n",
    "Consequences: It may lead to reduced sample size, potential bias if the data is not missing completely at random, and a decrease in statistical power.\n",
    "\n",
    "Pairwise Deletion:\n",
    "In pairwise deletion, each analysis is conducted using only the available data for the specific variables being analyzed. This means that for each specific analysis, cases with missing data for the variables in that analysis are excluded.\n",
    "Consequences: It allows you to retain more data and may provide insight into the patterns of missing data. However, it can result in different sample sizes for different analyses, and it can lead to biased results if the data is not missing at random.\n",
    "\n",
    "Imputation Methods:\n",
    "It involve filling in missing values with estimates. Common imputation methods include mean imputation, median imputation, regression imputation, or multiple imputations.\n",
    "Consequences: Imputation methods can help retain more data and maintain a consistent sample size across analyses. However, the choice of imputation method can impact the results and may introduce bias if the imputation model is misspecified or if the missing data is not missing at random.\n",
    "\n",
    "Mixed-Effects Models:\n",
    "Mixed-effects models (also known as hierarchical linear models or linear mixed models) are a more advanced approach that can handle missing data in a principled way by estimating the missing data points while considering the hierarchical structure of the data.\n",
    "Consequences: Mixed-effects models are a powerful approach for handling missing data in repeated measures ANOVA. They provide valid parameter estimates and are less likely to introduce bias compared to other methods."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8f7e8a4-f8ed-4bb7-beed-f850a3919f2b",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "4a49aab8-a254-4555-836b-50c9ed9131e2",
   "metadata": {},
   "source": [
    "Q8. What are some common post-hoc tests used after ANOVA, and when would you use each one? Provide an example of a situation where a post-hoc test might be necessary."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b29f493-a4d7-44db-8091-c17d54135cc9",
   "metadata": {},
   "source": [
    "Ans. Post-hoc tests are used in the context of Analysis of Variance (ANOVA) to make pairwise comparisons between groups when the ANOVA test indicates that there are statistically significant differences among three or more groups. These tests help identify which specific groups differ from each other. Several common post-hoc tests are available, and the choice of which one to use depends on the specific assumptions and needs of your analysis. Here are some common post-hoc tests and when you might use each one:\n",
    "\n",
    "Tukey's Honestly Significant Difference (HSD):\n",
    "Use Tukey's HSD when you want to control the family-wise error rate and maintain an overall Type I error rate at a specified level (e.g., 0.05). It is a conservative method that accounts for multiple comparisons.\n",
    "\n",
    "Example: You've conducted a one-way ANOVA to compare the performance of three different teaching methods, and the ANOVA indicates a significant difference. Tukey's HSD can help you determine which specific teaching methods are different from each other in terms of student test scores.\n",
    "\n",
    "Bonferroni Correction:\n",
    "Use Bonferroni correction when you want to control the family-wise error rate by adjusting the alpha level for each individual comparison. It is a more conservative method and helps reduce the likelihood of Type I errors when conducting multiple comparisons.\n",
    "\n",
    "Example: You're comparing the effectiveness of multiple marketing strategies, and the ANOVA shows a significant difference. You can use the Bonferroni correction to assess pairwise differences between each pair of marketing strategies while protecting against Type I errors.\n",
    "\n",
    "Scheffé's Method:\n",
    "Use Scheffé's method when you want to maintain a family-wise error rate at a specified level (e.g., 0.05) and your data do not meet the assumption of equal variances between groups. It is a more robust method but may be less powerful than Tukey's HSD.\n",
    "\n",
    "Example: You're comparing the effects of different drug treatments on a health outcome, and the ANOVA reveals significant differences. Scheffé's method can help you make pairwise comparisons among the drug treatments even when the variances in health outcomes are not equal between treatments.\n",
    "\n",
    "Dunnett's Test:\n",
    "Use Dunnett's test when you have a control group and you want to compare all other groups to the control group. It is particularly useful for situations where you are interested in treatment effects relative to a reference group.\n",
    "\n",
    "Example: In a clinical trial, you have a control group and several experimental groups receiving different drug treatments. Dunnett's test can help you determine which experimental groups have statistically significant differences compared to the control group.\n",
    "\n",
    "Games-Howell Test:\n",
    "Use the Games-Howell test when the assumption of equal variances is violated, and the group sizes are unequal. It is a robust method for making pairwise comparisons under such conditions.\n",
    "\n",
    "Example: You're comparing the performance of different departments in a company, and the ANOVA indicates significant differences. The Games-Howell test can be used to determine which specific departments differ from each other in terms of productivity, even when department sizes are not equal and variances are not equal."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50373352-a422-4421-aad3-4f3c9ff3368a",
   "metadata": {},
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "55a7a4ec-5e9e-4799-b2f1-dbe62dc7df64",
   "metadata": {},
   "source": [
    "Q9. A researcher wants to compare the mean weight loss of three diets: A, B, and C. They collect data from 50 participants who were randomly assigned to one of the diets. Conduct a one-way ANOVA using Python to determine if there are any significant differences between the mean weight loss of the three diets. Report the F-statistic and p-value, and interpret the results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "3109e863-402c-4ad2-9d62-269b1b6e7dc6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 0.23574581543475034\n",
      "P-value: 0.7904844382783518\n",
      "There is no statistically significant difference in mean weight loss among the three diets.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "diet_A = [1.2, 2.5, 3.1, 4.2, 2.9, 3.5, 1.8, 3.2, 2.7, 4.0,\n",
    "          1.3, 2.2, 3.4, 4.1, 2.7, 3.0, 1.9, 2.4, 3.0, 3.9,\n",
    "          1.5, 2.1, 3.2, 4.0, 2.5, 2.8, 1.7, 3.5, 2.3, 3.3]\n",
    "\n",
    "diet_B = [1.6, 2.4, 3.3, 3.8, 2.6, 3.6, 1.5, 2.8, 3.2, 4.2,\n",
    "          1.9, 2.7, 3.2, 3.9, 2.8, 3.1, 1.7, 2.5, 3.1, 4.1,\n",
    "          1.8, 2.9, 3.5, 4.0, 2.9, 3.4, 1.6, 2.6, 3.0, 3.8]\n",
    "\n",
    "diet_C = [1.3, 2.7, 3.0, 4.1, 2.8, 3.2, 1.6, 2.9, 2.5, 3.7,\n",
    "          1.7, 2.5, 3.3, 3.8, 2.5, 3.4, 1.8, 2.4, 3.3, 3.5,\n",
    "          1.4, 2.8, 3.1, 3.9, 2.6, 3.3, 1.5, 2.6, 2.9, 3.6]\n",
    "\n",
    "all_data = np.concatenate([diet_A, diet_B, diet_C])\n",
    "\n",
    "f_statistic, p_value = stats.f_oneway(diet_A, diet_B, diet_C)\n",
    "\n",
    "print(f\"F-statistic: {f_statistic}\")\n",
    "print(f\"P-value: {p_value}\")\n",
    "\n",
    "if p_value < 0.05:\n",
    "    print(\"There is a statistically significant difference in mean weight loss among the three diets.\")\n",
    "else:\n",
    "    print(\"There is no statistically significant difference in mean weight loss among the three diets.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b4ef1da-a508-4f51-a81b-66e189efa51a",
   "metadata": {},
   "source": [
    "Interpret the results:\n",
    "In the context of the one-way ANOVA test, you are assessing whether there are statistically significant differences in mean weight loss between the three diets (A, B, and C).\n",
    "\n",
    "F-statistic: This statistic tests the null hypothesis that the means of the three diets are equal. In this case, the F-statistic is a measure of the variability between the sample means relative to the variability within the samples. It quantifies whether the differences among the diets' means are larger than what you would expect by chance.\n",
    "\n",
    "P-value: The p-value associated with the F-statistic indicates the probability of observing the results (or more extreme results) if there were no true differences among the diets. In this example, the p-value is what you will use to make your conclusion.\n",
    "\n",
    "If the p-value is less than your chosen significance level (e.g., 0.05), you can conclude that there are statistically significant differences in mean weight loss among the three diets. If the p-value is greater than 0.05, you would fail to reject the null hypothesis, indicating that there are no significant differences."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9e1e4dac-d420-48f4-8d07-fb95016ecf59",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "d0af47c5-1682-4067-8c0a-7aafbcc3a2a5",
   "metadata": {},
   "source": [
    "Q10. A company wants to know if there are any significant differences in the average time it takes to complete a task using three different software programs: Program A, Program B, and Program C. They randomly assign 30 employees to one of the programs and record the time it takes each employee to complete the task. Conduct a two-way ANOVA using Python to determine if there are any main effects or interaction effects between the software programs and employee experience level (novice vs. experienced). Report the F-statistics and p-values, and interpret the results."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "f10235fa-a81a-4e85-a7ff-e7959767a47e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                              sum_sq    df         F    PR(>F)\n",
      "C(Software)                 2.384667   2.0  1.254427  0.303280\n",
      "C(Experience)               0.867000   1.0  0.912151  0.349066\n",
      "C(Software):C(Experience)   0.626000   2.0  0.329300  0.722626\n",
      "Residual                   22.812000  24.0       NaN       NaN\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from scipy import stats\n",
    "from statsmodels.formula.api import ols\n",
    "from statsmodels.stats.anova import anova_lm\n",
    "\n",
    "data = {\n",
    "    'Software': ['A', 'B', 'C'] * 10,\n",
    "    'Experience': ['Novice'] * 15 + ['Experienced'] * 15,\n",
    "    'Time': [12.5, 14.2, 13.9, 14.1, 15.0, 13.8, 14.9, 12.8, 13.6, 13.2,\n",
    "             11.8, 13.5, 14.3, 13.9, 14.5, 16.0, 13.6, 12.7, 14.0, 13.2,\n",
    "             15.8, 14.2, 13.7, 14.9, 13.1, 12.6, 14.0, 13.7, 14.4, 15.2]\n",
    "}\n",
    "\n",
    "df = pd.DataFrame(data)\n",
    "\n",
    "model = ols('Time ~ C(Software) * C(Experience)', data=df).fit()\n",
    "\n",
    "anova_table = anova_lm(model, typ=2)\n",
    "\n",
    "print(anova_table)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e9a6eb9-3df3-4a2f-b49e-5d4db441e2b9",
   "metadata": {},
   "source": [
    "Interpret the results:\n",
    "The ANOVA table will provide F-statistics and p-values for the main effects of software programs, the main effects of employee experience levels, and the interaction effect between software programs and experience levels.\n",
    "\n",
    "Main Effects:\n",
    "The main effect of Software tests whether there are significant differences in task completion times among the three software programs (A, B, C).\n",
    "The main effect of Experience tests whether there are significant differences in task completion times between novice and experienced employees.\n",
    "\n",
    "Interaction Effect:\n",
    "The interaction effect between Software and Experience tests whether the combination of software programs and experience levels has a significant impact on task completion times.\n",
    "You can interpret the results based on the F-statistics and p-values in the ANOVA table. If the p-value for a main effect or interaction effect is less than your chosen significance level (e.g., 0.05), you can conclude that there is a statistically significant effect."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e716e33c-16e0-40f0-847a-03167202d4ec",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "bc37ddf9-71e5-4680-a8d5-8677448c1066",
   "metadata": {},
   "source": [
    "Q11. An educational researcher is interested in whether a new teaching method improves student test scores. They randomly assign 100 students to either the control group (traditional teaching method) or the experimental group (new teaching method) and administer a test at the end of the semester. Conduct a two-sample t-test using Python to determine if there are any significant differences in test scores between the two groups. If the results are significant, follow up with a post-hoc test to determine which group(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "5dfd9189-2c07-45ce-a20a-607a84274c3e",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "t-statistic: -6.246645356495108\n",
      "P-value: 5.32497691220266e-08\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "control_group = [80, 85, 88, 82, 79, 83, 86, 81, 78, 84, 87, 82, 79, 85, 88, 82, 80, 83, 86, 81, 78, 84, 87, 82, 79, 85, 88, 82, 80, 83]\n",
    "experimental_group = [85, 90, 92, 86, 84, 88, 91, 87, 83, 89, 92, 87, 85, 90, 92, 86, 84, 88, 91, 87, 83, 89, 92, 87, 85, 90, 92, 86, 84, 88]\n",
    "\n",
    "t_statistic, p_value = stats.ttest_ind(control_group, experimental_group)\n",
    "\n",
    "print(f\"t-statistic: {t_statistic}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e34370-62dc-4076-bff7-0d6c792ab1ef",
   "metadata": {},
   "source": [
    "Interpret the results:\n",
    "If the p-value is less than your chosen significance level (e.g., 0.05), you can conclude that there is a statistically significant difference in test scores between the control and experimental groups. If the results are significant, you can proceed with post-hoc tests for pairwise comparisons."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dbe84420-5d91-4f9d-b43d-77bc047ba325",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "149a206e-b849-44be-97ea-3bce8aed5fe0",
   "metadata": {},
   "source": [
    "Q12. A researcher wants to know if there are any significant differences in the average daily sales of three retail stores: Store A, Store B, and Store C. They randomly select 30 days and record the sales for each store on those days. Conduct a repeated measures ANOVA using Python to determine if there are any significant differences in sales between the three stores. If the results are significant, follow up with a post-hoc test to determine which store(s) differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "14fff23c-a7bf-4b0c-88cb-26615aec7cca",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "F-statistic: 549.6477192799659\n",
      "P-value: 4.382259159504167e-50\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from scipy import stats\n",
    "\n",
    "store_A_sales = [1500, 1550, 1600, 1450, 1520, 1570, 1540, 1470, 1600, 1505, 1560, 1490, 1580, 1610, 1530, 1485, 1575, 1590, 1460, 1555, 1525, 1595, 1545, 1510, 1465, 1480, 1565, 1605, 1500, 1475]\n",
    "store_B_sales = [1350, 1380, 1365, 1335, 1320, 1375, 1390, 1355, 1325, 1360, 1330, 1345, 1385, 1360, 1340, 1315, 1350, 1375, 1325, 1355, 1370, 1365, 1340, 1380, 1350, 1365, 1335, 1320, 1375, 1355]\n",
    "store_C_sales = [1250, 1280, 1265, 1235, 1220, 1275, 1290, 1255, 1225, 1260, 1230, 1245, 1285, 1260, 1240, 1215, 1250, 1275, 1225, 1255, 1270, 1265, 1240, 1280, 1250, 1265, 1235, 1220, 1275, 1255]\n",
    "\n",
    "f_statistic, p_value = stats.f_oneway(store_A_sales, store_B_sales, store_C_sales)\n",
    "\n",
    "print(f\"F-statistic: {f_statistic}\")\n",
    "print(f\"P-value: {p_value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1455ed21-267d-4d24-8c27-83c5040d9328",
   "metadata": {},
   "source": [
    "Interpret the results:\n",
    "If the p-value is less than your chosen significance level (e.g., 0.05), you can conclude that there is a statistically significant difference in average daily sales between the three stores. If the results are significant, you can proceed with post-hoc tests, such as Tukey's HSD or Bonferroni correction, to determine which specific stores differ significantly from each other."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "50b2c896-6047-4df9-ae6b-eb9d56540805",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
